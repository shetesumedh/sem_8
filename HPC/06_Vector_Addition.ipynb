{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aef45c25",
      "metadata": {
        "id": "aef45c25"
      },
      "source": [
        "Problem Statment: Write a CUDA Program for Addition of two large vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "349e8ab9",
      "metadata": {
        "id": "349e8ab9"
      },
      "source": [
        "-- Program Overview --\n",
        "\n",
        "This program performs vector addition using CUDA (Compute Unified Device Architecture) to run parallel code on an NVIDIA GPU. It:\n",
        "    Generates two large vectors A and B with random integers.\n",
        "    Adds the vectors element-wise in parallel on the GPU.\n",
        "    Stores the result in vector C.\n",
        "    Writes all three vectors to a text file in tabular format."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1952220d",
      "metadata": {
        "id": "1952220d"
      },
      "source": [
        "1. CUDA Basics\n",
        "    CUDA is a parallel computing platform by NVIDIA that allows using the GPU (Graphics Processing Unit) for general-purpose computing.\n",
        "    It uses extensions to C/C++ to run functions (called kernels) in parallel across many GPU threads.\n",
        "\n",
        "2. Kernels\n",
        "    A kernel is a function that runs on the GPU. It’s called with <<<blocks, threads>>> syntax.\n",
        "    Each thread computes a part of the problem—in this case, one index of the vector sum.\n",
        "\n",
        "3. Memory Management\n",
        "    Host (CPU) and Device (GPU) have separate memory spaces.\n",
        "    You must explicitly allocate GPU memory (cudaMalloc) and copy data between host and device (cudaMemcpy).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9ee69b6",
      "metadata": {
        "id": "a9ee69b6"
      },
      "source": [
        "Explanation of How the Program Works\n",
        "\n",
        "Step 1: Initialization\n",
        "- N = 2^20 = 1,048,576 defines the size of the vectors.\n",
        "- Host vectors h_A, h_B, h_C are created using std::vector.\n",
        "- Random values (0–100) are assigned to h_A and h_B.\n",
        "\n",
        "Step 2: GPU Memory Allocation\n",
        "- cudaMalloc allocates memory for d_A, d_B, d_C on the GPU.\n",
        "- cudaMemcpy copies vectors h_A and h_B to the GPU.\n",
        "\n",
        "Step 3: Kernel Launch\n",
        "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "- Launches the GPU kernel vectorAdd.\n",
        "- Each thread computes: C[i] = A[i] + B[i].\n",
        "\n",
        "Step 4: Error Checking & Synchronization\n",
        "- cudaGetLastError() checks if kernel launch was successful.\n",
        "- cudaDeviceSynchronize() ensures the kernel execution is finished.\n",
        "\n",
        "Step 5: Copy Back Result\n",
        "- cudaMemcpy copies result d_C to host vector h_C.\n",
        "\n",
        "Step 6: Output to File\n",
        "- The result is saved to vector_sum_output.txt in a well-formatted table using std::ofstream.\n",
        "\n",
        "Step 7: Memory Cleanup\n",
        "- Frees GPU memory using cudaFree."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9807fe70",
      "metadata": {
        "id": "9807fe70"
      },
      "source": [
        "Important CUDA Functions Used\n",
        "\n",
        "1. cudaMalloc:\tAllocates memory on GPU\n",
        "2. cudaMemcpy:\tCopies data between CPU and GPU\n",
        "3. cudaFree:\tFrees allocated GPU memory\n",
        "4. cudaGetLastError:\tChecks for kernel launch errors\n",
        "5. cudaDeviceSynchronize:\tWaits for all GPU threads to complete"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46e71ab8",
      "metadata": {
        "id": "46e71ab8"
      },
      "source": [
        "The below code is for the vector addition, which we have to perform on the Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5b4a22ee",
      "metadata": {
        "id": "5b4a22ee"
      },
      "outputs": [],
      "source": [
        "code = r'''\n",
        "#include <iostream>\n",
        "#include <fstream>\n",
        "#include <vector>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "#include <iomanip>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void vectorAdd(const int *A, const int *B, int *C, int N) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < N) {\n",
        "        C[idx] = A[idx] + B[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "void checkCudaError(cudaError_t err, const char *msg) {\n",
        "    if (err != cudaSuccess) {\n",
        "        std::cerr << \"CUDA Error: \" << msg << \": \" << cudaGetErrorString(err) << std::endl;\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1 << 20;\n",
        "    size_t size = N * sizeof(int);\n",
        "\n",
        "    std::vector<int> h_A(N), h_B(N), h_C(N);\n",
        "\n",
        "    srand(static_cast<unsigned>(time(nullptr)));\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        h_A[i] = rand() % 101;  // integers from 0 to 100\n",
        "        h_B[i] = rand() % 101;\n",
        "    }\n",
        "\n",
        "    int *d_A = nullptr, *d_B = nullptr, *d_C = nullptr;\n",
        "    checkCudaError(cudaMalloc(&d_A, size), \"Allocating d_A\");\n",
        "    checkCudaError(cudaMalloc(&d_B, size), \"Allocating d_B\");\n",
        "    checkCudaError(cudaMalloc(&d_C, size), \"Allocating d_C\");\n",
        "\n",
        "    checkCudaError(cudaMemcpy(d_A, h_A.data(), size, cudaMemcpyHostToDevice), \"Copying h_A\");\n",
        "    checkCudaError(cudaMemcpy(d_B, h_B.data(), size, cudaMemcpyHostToDevice), \"Copying h_B\");\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    checkCudaError(cudaGetLastError(), \"Kernel launch\");\n",
        "    checkCudaError(cudaDeviceSynchronize(), \"Kernel execution\");\n",
        "\n",
        "    checkCudaError(cudaMemcpy(h_C.data(), d_C, size, cudaMemcpyDeviceToHost), \"Copying result\");\n",
        "\n",
        "    // Save to file in column format\n",
        "    std::ofstream outFile(\"vector_sum_output.txt\");\n",
        "    if (!outFile.is_open()) {\n",
        "        std::cerr << \"Error opening output file!\" << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    outFile << std::setw(10) << \"A[i]\"\n",
        "            << std::setw(10) << \"B[i]\"\n",
        "            << std::setw(15) << \"C[i] = A + B\" << \"\\n\";\n",
        "    outFile << std::string(35, '-') << \"\\n\";\n",
        "\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        outFile << std::setw(10) << h_A[i]\n",
        "                << std::setw(10) << h_B[i]\n",
        "                << std::setw(15) << h_C[i] << \"\\n\";\n",
        "    }\n",
        "\n",
        "    outFile.close();\n",
        "\n",
        "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "'''\n",
        "with open(\"vector_add_random.cu\", \"w\") as f:\n",
        "    f.write(code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "024ac7ff",
      "metadata": {
        "id": "024ac7ff"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 vector_add_random.cu -o vector_add_random\n",
        "!./vector_add_random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 20 vector_sum_output.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C9JPaVIGCxI",
        "outputId": "1a17f229-5683-4eab-8566-4270afb2957c"
      },
      "id": "8C9JPaVIGCxI",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      A[i]      B[i]   C[i] = A + B\n",
            "-----------------------------------\n",
            "        67        65            132\n",
            "        20        95            115\n",
            "        49        98            147\n",
            "        16        93            109\n",
            "        66        20             86\n",
            "         7        58             65\n",
            "        26        46             72\n",
            "        25        63             88\n",
            "        83        99            182\n",
            "        64        76            140\n",
            "        64        59            123\n",
            "        46         5             51\n",
            "        55        24             79\n",
            "         1         3              4\n",
            "        72        89            161\n",
            "        62         5             67\n",
            "        19        48             67\n",
            "       100        35            135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tqZaWSfDGD8Y"
      },
      "id": "tqZaWSfDGD8Y",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}